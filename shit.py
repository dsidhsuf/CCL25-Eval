import json
import torch
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    EarlyStoppingCallback,
)
from datasets import Dataset
from tqdm import tqdm

# âœ… æ¨¡å‹é…ç½®
model_name = "Langboat/mengzi-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# âœ… å¼ºåˆ¶ä½¿ç”¨GPU
if not torch.cuda.is_available():
    raise RuntimeError("å¿…é¡»ä½¿ç”¨GPUè®­ç»ƒï¼Œè¯·æ£€æŸ¥CUDAç¯å¢ƒ")
device = torch.device("cuda")
model = model.to(device)

# âœ… åŠ è½½ JSON æ•°æ®
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# âœ… æ¸…æ´—æ•°æ®ï¼ˆä»…ä¿ç•™æœ‰ content å’Œéç©º output çš„æ ·æœ¬ï¼‰
def clean_data(data):
    return [d for d in data if d.get("content") and d.get("output") and d["output"].strip()]

# âœ… åŠ è½½å¹¶å¤„ç†è®­ç»ƒæ•°æ®
all_data = clean_data(load_json("train.json"))

# âœ… è‡ªåŠ¨åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ9:1ï¼‰
train_data, val_data = train_test_split(all_data, test_size=0.1, random_state=42)

# âœ… åŠ è½½æµ‹è¯•é›†
test_data = load_json("test1.json")

# âœ… è½¬æ¢ä¸º Huggingface Datasets
train_ds = Dataset.from_list(train_data)
val_ds = Dataset.from_list(val_data)
test_ds = Dataset.from_list(test_data)

# âœ… åˆ†è¯å‡½æ•°
def tokenize_function(example):
    model_inputs = tokenizer(
        example["content"],
        max_length=256,
        padding="max_length",
        truncation=True,
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            example["output"],
            max_length=256,
            padding="max_length",
            truncation=True,
        )
    model_inputs["labels"] = [
        (token if token != tokenizer.pad_token_id else -100)
        for token in labels["input_ids"]
    ]
    return model_inputs

# âœ… åˆ†è¯æ˜ å°„
tokenized_train = train_ds.map(tokenize_function)
tokenized_val = val_ds.map(tokenize_function)

# âœ… æ•°æ®æ‰“åŒ…å™¨
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# âœ… è®­ç»ƒå‚æ•°
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="steps",
    eval_steps=100,
    logging_steps=50,
    save_steps=100,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=8,
    weight_decay=0.01,
    save_total_limit=2,
    predict_with_generate=True,
    fp16=True,  # âœ… å¯ç”¨åŠç²¾åº¦è®­ç»ƒ
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
)

# âœ… åˆå§‹åŒ– Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)

# âœ… å¼€å§‹è®­ç»ƒ
trainer.train()

# âœ… ä¿å­˜æ¨¡å‹
model.save_pretrained("./saved_model")
tokenizer.save_pretrained("./saved_model")

# ==========================================
# âœ… æ¨ç†æµ‹è¯•é›†å¹¶ä¿å­˜ç»“æœåˆ° demo.txt
# ==========================================

print("ğŸ” å¼€å§‹æ¨ç†æµ‹è¯•é›†...")

model.eval()
with open("demo.txt", "w", encoding="utf-8") as f:
    for item in tqdm(test_data):
        input_text = item["content"]
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            max_length=256,
            truncation=True,
            padding="max_length"
        ).to(device)

        output_ids = model.generate(
            **inputs,
            max_length=256,
            num_beams=5,
            early_stopping=True
        )
        pred = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

        # âœ… è‡ªåŠ¨è¡¥å…¨ [END]
        if not pred.endswith("[END]"):
            pred += " [END]"

        f.write(pred + "\n")
